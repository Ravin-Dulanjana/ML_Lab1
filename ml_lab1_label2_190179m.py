# -*- coding: utf-8 -*-
"""ML_Lab1_Label2_190179M.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17lvY36a_w2kv8aUYrSJ_3N1TStVS4Xry
"""

from google.colab import drive
drive.mount('/content/gdrive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor

trainData = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ML/Lab1/train.csv')
validData = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ML/Lab1/valid.csv')
testData = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ML/Lab1/test.csv')

trainData.head()

# Verify the presence of null values within the train data
trainNullCounts = trainData.isnull().sum()
print("Null counts in Train Data: \n {}".format(trainNullCounts))

# Remove rows with null values in the last four columns (target labels) of the train data
trainData = trainData.dropna(subset=trainData.columns[-4:], how='any')

# Replace null values with mean in datasets
trainData = trainData.fillna(trainData.mean())
validData = validData.fillna(validData.mean())
testData = testData.fillna(testData.mean())

trainData.head()

# Separate features and labels in datasets
trainFeatures = trainData.iloc[:, :-4]
trainLabels = trainData.iloc[:, -4:]
validFeatures = validData.iloc[:, :-4]
validLabels = validData.iloc[:, -4:]
testFeatures = testData.iloc[:, :-4]
testLabels = testData.iloc[:, -4:]

# Extract the second label of the datasets
trainLabel2 = trainLabels.iloc[:,1]
validLabel2 = validLabels.iloc[:,1]
testLabel2 = testLabels.iloc[:,1]

"""Predicting Label 2 without Feature Engineering"""

# Make a copy features and labels in datasets
trainFeaturesCopy = trainFeatures.copy()
trainLabelsCopy = trainLabels.copy()
validFeaturesCopy = validFeatures.copy()
validLabelsCopy = validLabels.copy()
testFeaturesCopy = testFeatures.copy()
testLabelsCopy = testLabels.copy()

# Make a copy of the second label of the datasets
trainLabel2Copy = trainLabel2.copy()
validLabel2Copy = validLabel2.copy()
testLabel2Copy = testLabel2.copy()

testData.head()

# Standardize features
scaler = StandardScaler()
trainFeaturesCopy = scaler.fit_transform(trainFeaturesCopy)
validFeaturesCopy = scaler.transform(validFeaturesCopy)
testFeaturesCopy = scaler.transform(testFeaturesCopy)

model = KNeighborsRegressor()
model.fit(trainFeaturesCopy, trainLabel2Copy)

# Predict on train data
testPredictionsBase = model.predict(trainFeaturesCopy)
trainMSE = mean_squared_error(trainLabel2Copy, testPredictionsBase)
trainr2s = r2_score(trainLabel2Copy, testPredictionsBase)
print(f"Metrics for KNeighborsRegressor on Train data:")
print(f"  - Mean Squared Error: {trainMSE:.2f}")
print(f"  - R2 Score: {trainr2s:.2f}\n")

# Predict on validation data
validPredictionsBase = model.predict(validFeaturesCopy)
validMSE = mean_squared_error(validLabel2Copy, validPredictionsBase)
validr2s = r2_score(validLabel2Copy, validPredictionsBase)
print(f"Metrics for KNeighborsRegressor on Validation data:")
print(f"  - Mean Squared Error: {validMSE:.2f}")
print(f"  - R2 Score: {validr2s:.2f}\n")

# Predict on test data
testPredictionsBase = model.predict(testFeaturesCopy)

"""Predicting Label 2 with Feature Engineering"""

# Plotting the distribution of trainLabel2
labels, counts = np.unique(trainLabel2, return_counts=True)

plt.figure(figsize=(10, 6))
plt.xticks(labels)
plt.bar(labels, counts)
plt.xlabel('Target Label 2')
plt.ylabel('Frequency')
plt.title('Frequency of Target Label 2')
plt.show()

#Compute the correlation matrix and create a heatmap
correlationMatrix = trainFeatures.corr()
mask = np.triu(np.ones_like(correlationMatrix))
plt.figure(figsize=(12, 12))
sns.heatmap(correlationMatrix, cmap='coolwarm', center=0, mask=mask)
plt.title("Correlation Matrix")
plt.show()

# Determine highly correlated features within the training dataset
correlationThreshold = 0.8
highlyCorrelated = set()
for i in range(len(correlationMatrix.columns)):
    for j in range(i):
        if abs(correlationMatrix.iloc[i, j]) > correlationThreshold:
            colname = correlationMatrix.columns[i]
            highlyCorrelated.add(colname)
print(highlyCorrelated)

# Exclude features that were previously identified as having high correlation from all datasets
trainFeatures = trainFeatures.drop(columns=highlyCorrelated)
validFeatures = validFeatures.drop(columns=highlyCorrelated)
testFeatures = testFeatures.drop(columns=highlyCorrelated)

# Display the filtered feature counts
print("Train features after filtering: {}".format(trainFeatures.shape))
print("Valid features after filtering: {}".format(validFeatures.shape))
print("Test features after filtering: {}".format(testFeatures.shape))

# Detect the attributes strongly linked to the target variable by analyzing the training dataset
correlationWithTarget = trainFeatures.corrwith(trainLabel2)
correlationThreshold = 0.06
highlyCorrelatedFeatures = correlationWithTarget[correlationWithTarget.abs() > correlationThreshold]
print(highlyCorrelatedFeatures)

# Drop the features with low correlation in the datasets
trainFeatures = trainFeatures[highlyCorrelatedFeatures.index]
validFeatures = validFeatures[highlyCorrelatedFeatures.index]
testFeatures = testFeatures[highlyCorrelatedFeatures.index]

# Display the filtered feature counts  of the datasets
print("Train features after filtering: {}".format(trainFeatures.shape))
print("Valid features after filtering: {}".format(validFeatures.shape))
print("Test features after filtering: {}".format(testFeatures.shape))

# Standardize the features
scaler = StandardScaler()
standardizedTrainFeatures = scaler.fit_transform(trainFeatures)
standardizedValidFeatures = scaler.transform(validFeatures)
standardizedTestFeatures = scaler.transform(testFeatures)

"""Feature Extraction"""

varianceThreshold = 0.95

# Apply PCA with the determined no. of components
pca = PCA(n_components=varianceThreshold, svd_solver='full')
pcaTrainResult = pca.fit_transform(standardizedTrainFeatures)
pcaValidResult = pca.transform(standardizedValidFeatures)
pcaTestResult = pca.transform(standardizedTestFeatures)

# Explained variance ratio after dimensionality reduction
explainedVarianceRatioReduced = pca.explained_variance_ratio_
print("Explained Variance Ratio following Dimensionality Reduction:", explainedVarianceRatioReduced)

# Visualize Variance Explained Ratio
plt.figure(figsize=(18, 10))
plt.bar(range(1, pcaTrainResult.shape[1] + 1), explainedVarianceRatioReduced)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance Ratio for Each Principal Component (Reduced Dimensionality)')
plt.show()

# Display the reduced feature matrices
print("Shape of the Reduced Training Feature Matrix: {}".format(pcaTrainResult.shape))
print("Shape of the Reduced Validation Feature Matrix: {}".format(pcaValidResult.shape))
print("Shape of the Reduced Testing Feature Matrix: {}".format(pcaTestResult.shape))

"""Model Selection"""

classificationModels = [
    # ('Decision Tree', DecisionTreeRegressor()),
    ('K Neighbors', KNeighborsRegressor()),
    # ('Linear Regression', LinearRegression()),
    # ('Random Forest', RandomForestRegressor()),
    # ('XGBoost', XGBRegressor())
]

# KNN Regressor is the best model

featureNum = pcaTrainResult.shape[1]
print(f"No. of features: {featureNum}\n")

# Train and evaluate each classification model
for modelName, model in classificationModels:
    model.fit(pcaTrainResult, trainLabel2)
    predictionTrain = model.predict(pcaTrainResult)
    mse = mean_squared_error(trainLabel2, predictionTrain)
    r2s = r2_score(trainLabel2, predictionTrain)

    print(f"Metrics for {modelName} on train data:")
    print(f"  - Mean Squared Error: {mse:.2f}")
    print(f"  - R2 Score: {r2s:.2f}\n")

    # Predict on the validation data
    predictionValid = model.predict(pcaValidResult)
    mse = mean_squared_error(validLabel2, predictionValid)
    r2s = r2_score(validLabel2, predictionValid)

    print(f"Metrics for {modelName} on validation data:")
    print(f"  - Mean Squared Error: {mse:.2f}")
    print(f"  - R2 Score: {r2s:.2f}\n")

    # Predict on the test data
    predictionTest = model.predict(pcaTestResult)

"""Generating CSV File"""

# define method to create the dataframe and save in a csv file
def generateCsv(features, predictionsBeforeFeatureEng, predictionsAfterFeatureEng, destination):
  featureCount = features.shape[1]
  headerRow = [f"new_feature_{i}" for i in range(1, featureCount+1)]
  df = pd.DataFrame(features, columns  = headerRow)
  df.insert(loc=0, column='Predicted labels before feature engineering', value=predictionsBeforeFeatureEng)
  df.insert(loc=1, column='Predicted labels after feature engineering', value=predictionsAfterFeatureEng)
  df.insert(loc=2, column='No. of new features', value=np.repeat(featureCount, features.shape[0]))
  df.to_csv(destination, index=False)

generateCsv(pcaTestResult, testPredictionsBase, predictionTest, '/content/gdrive/MyDrive/Colab Notebooks/ML/Lab1/190179M_label_2.csv')